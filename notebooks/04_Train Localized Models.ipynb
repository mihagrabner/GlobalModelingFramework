{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK DESCRIPTION\n",
    "The goal of this notebook is to train localized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.regularizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.path.dirname(os.getcwd())\n",
    "\n",
    "# run tensorflow_helper_func.py\n",
    "tensorflow_helper_func_path =    '\"{}/modules/tensorflow_helper_func.py\"'.format(cwd)\n",
    "%run $tensorflow_helper_func_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_gen = cwd + \"/generated_data/\"\n",
    "folder_mod_global = cwd + \"/models/global N-BEATS-exog/\"\n",
    "folder_mod_local = cwd + \"/models/localized N-BEATS-exog/\"\n",
    "\n",
    "train_dict = pickle.load(open(folder_gen + \"train_dict.p\",'rb'))\n",
    "val_dict = pickle.load(open(folder_gen + \"val_dict.p\",'rb'))\n",
    "\n",
    "# LOAD CLUSTERING RESULTS\n",
    "clust_all = pd.read_csv(folder_gen + \"cluster_data.csv\", index_col=\"ts_id\")\n",
    "clust_all.columns = clust_all.columns.astype(int)\n",
    "\n",
    "# OTHER\n",
    "freq = \"30min\"\n",
    "periods = 48\n",
    "idx = pd.date_range(\"2009-07-20\", \"2009-12-07\", freq=\"30min\", closed=\"left\")\n",
    "y_cols = [\"H_{}\".format(i) for i in range(1, periods+1)]\n",
    "\n",
    "train_idx = pd.date_range(\"2009-07-27\", \"2010-07-12\", freq=\"30min\", closed=\"left\", name=\"timestamp\")\n",
    "val_idx = pd.date_range(\"2010-07-12\", freq=\"30min\", periods=periods*7*12, name=\"timestamp\")\n",
    "test_idx = pd.date_range(\"2010-10-04\", freq=\"30min\", periods=periods*7*12, name=\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Create dict. <i>clusters_dict</i> which holds information about clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = {}\n",
    "\n",
    "for n_clusters in range(2, 21, 1):\n",
    "    n_clusters_dict = {}\n",
    "    for clust_n in range(n_clusters):\n",
    "        clust_list = clust_all.loc[:, n_clusters][clust_all.loc[:, n_clusters] == clust_n].index.tolist()\n",
    "        n_clusters_dict[\"clust={}\".format(clust_n)] = clust_list\n",
    "        \n",
    "    clusters_dict[n_clusters] = n_clusters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame <i> clust_all </i> stores clustering results. Rows represent ts_id's and columns represent cluster labels. E.g. column 3 denotes results when using 3 cluster centroids (k=3), therefore resulting labels for each ts_id can be 0, 1, or 2 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  \\\n",
       "ts_id                                                                           \n",
       "0       1   0   1   1   1   3   2   1   8   0   1   1  10   2   2   1   1   6   \n",
       "1       1   0   1   1   1   3   2   1   8   0   1   1  10   2   2   1   1   6   \n",
       "2       1   0   1   1   1   3   2   1   9   5   8  12   0   2  11   1   1   6   \n",
       "3       1   0   1   1   1   3   2   7   9   5   8  12   0   9  11   6  10  17   \n",
       "4       0   1   0   2   3   0   1   7   2   3   7   6   8  12  10  15   5  18   \n",
       "\n",
       "       20  \n",
       "ts_id      \n",
       "0      12  \n",
       "1      12  \n",
       "2      12  \n",
       "3      11  \n",
       "4      14  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary <i> clusters_dict </i> is created from clust_all. Keys represent number of clusters used as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 3, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When having 3 clusters (k=3), we have 3 subsets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clust=0', 'clust=1', 'clust=2'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_dict[3].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cluster = 0, following are ts_id's that belong to this cluster subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_dict[3]['clust=0'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Create localized models\n",
    "- Fine-tune initial global model on a subsets of original set of time series to create localized models.\n",
    "- To check whether models need additional training, you can explore results stored in val_results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# TRAINING PARAMS\n",
    "lr = 0.0001\n",
    "batch_size = 256\n",
    "\n",
    "n_steps_per_epoch = 50\n",
    "\n",
    "# MODEL PARAMS\n",
    "params_dict = {\"input_size\": [7*periods, 12+7+48],\n",
    "               \"output_size\": periods,\n",
    "               \"block_layers\": 3,\n",
    "               \"hidden_units\": 512, \n",
    "               \"n_blocks\": 3,\n",
    "               \"block_sharing\": False}\n",
    "\n",
    "# FINE-TUNE ON SUB-SETS OF ORIGINAL SET\n",
    "for n_clusters in clusters_dict:\n",
    "    for clust_set in clusters_dict[n_clusters]:\n",
    "        print(\"n_clusters: {}, clust_set: {} ------------------- \\n\".format(n_clusters, clust_set))\n",
    "\n",
    "        # GET ts ids for a specific cluster (subset)\n",
    "        ts_ids_list = clusters_dict[n_clusters][clust_set]\n",
    "\n",
    "        # INDICES for all ts\n",
    "        train_idx_all, train_ids = create_ts_idx(train_idx, ts_ids_list, train_dict)\n",
    "        val_idx_all, val_ids = create_ts_idx(val_idx, ts_ids_list, val_dict)\n",
    "        test_idx_all, test_ids = create_ts_idx(test_idx, ts_ids_list)\n",
    "\n",
    "        # DATA GENERATORS\n",
    "        train_generator = TSGenerator(set_type=\"train\",\n",
    "                                      batch_size=batch_size, \n",
    "                                      n_steps_per_epoch=n_steps_per_epoch,\n",
    "                                      ts_ids_list=ts_ids_list)\n",
    "        val_generator = TSGenerator(set_type=\"val\", ts_ids_list=ts_ids_list)\n",
    "        test_generator = TSGenerator(set_type=\"test\", ts_ids_list=ts_ids_list)\n",
    "\n",
    "        # CREATE NN MODEL\n",
    "        model = NBeats_exog(params_dict)\n",
    "        optimizer = Adam(lr=lr)\n",
    "        model.compile(optimizer, loss=\"mae\", \n",
    "                      metrics=[\"mae\"])\n",
    "\n",
    "        # CALLBACKS\n",
    "        csvlogger = CSVLogger(folder_mod_local + 'temp_log.csv')\n",
    "        weights_path = folder_mod_local + \"weights/val_best_weights, n_clusters={}, {}.h5\".format(n_clusters, clust_set)\n",
    "        save_val_weights = ModelCheckpoint(weights_path, monitor=\"val_mae\", save_best_only=True)\n",
    "        callbacks = [csvlogger] + [save_val_weights]\n",
    "\n",
    "        # TRAINING\n",
    "        ## load global model weights\n",
    "        model.load_weights(folder_mod_global + \"val_best_weights-global.h5\")\n",
    "        history = model.fit(train_generator,\n",
    "                            validation_data=val_generator,\n",
    "                            verbose=1,\n",
    "                            epochs=100,\n",
    "                            callbacks=callbacks)\n",
    "\n",
    "        results_val_all = []\n",
    "        results_val = pd.DataFrame(history.history)\n",
    "        results_val.index = results_val.index + 1\n",
    "        results_val_all.append(results_val)\n",
    "        best_epoch = results_val.val_mae.idxmin()\n",
    "        \n",
    "        # you could also use tf.keras.callbacks.EarlyStopping, \n",
    "        # but this still worked well.\n",
    "        for i in range(5):\n",
    "            if best_epoch > 70:\n",
    "                model.load_weights(weights_path)\n",
    "                history = model.fit(train_generator,\n",
    "                                    validation_data=val_generator,\n",
    "                                    verbose=1,\n",
    "                                    epochs=100,\n",
    "                                    callbacks=callbacks)\n",
    "\n",
    "                results_val = pd.DataFrame(history.history)\n",
    "                results_val.index = results_val.index + 1\n",
    "                results_val_all.append(results_val)\n",
    "                best_epoch = results_val.val_mae.idxmin()\n",
    "            else: break\n",
    "\n",
    "        results_val_all = pd.concat(results_val_all).reset_index(drop=True)\n",
    "        results_val_all.index = results_val_all.index + 1\n",
    "\n",
    "        # INFERENCE\n",
    "        ## load best weights\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "        ## predict on val\n",
    "        y_val_pred = model.predict(val_generator)\n",
    "        y_val_pred = pd.DataFrame(y_val_pred, index=val_idx_all, columns=y_cols)\n",
    "        y_val_pred = pd.concat([val_ids, y_val_pred], axis=1)\n",
    "\n",
    "        ## predict on test\n",
    "        y_test_pred = model.predict(test_generator)\n",
    "        y_test_pred = pd.DataFrame(y_test_pred, index=test_idx_all, columns=y_cols)\n",
    "        y_test_pred = pd.concat([test_ids, y_test_pred], axis=1)\n",
    "\n",
    "        # SAVE\n",
    "        results_val_all.to_pickle(folder_mod_local + \"val_results/val_results, n_clusters={}, {}.p\".format(n_clusters, clust_set))\n",
    "        y_val_pred.clip(0).to_pickle(folder_mod_local + \"y_val_pred/y_val_pred, n_clusters={}, {}.p\".format(n_clusters, clust_set))\n",
    "        y_test_pred.clip(0).to_pickle(folder_mod_local + \"y_test_pred/y_test_pred, n_clusters={}, {}.p\".format(n_clusters, clust_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
