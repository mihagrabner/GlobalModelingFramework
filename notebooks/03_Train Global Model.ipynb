{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK DESCRIPTION\n",
    "The goal of this notebook is to train initial global model on a whole set of input time series. Extended N-BEATS architecture and data generator (which creates batches of samples during training) functions are loaded from \"/modules/tensorflow_helper_func.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.regularizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.path.dirname(os.getcwd())\n",
    "\n",
    "# run tensorflow_helper_func.py\n",
    "tensorflow_helper_func_path =    '\"{}/modules/tensorflow_helper_func.py\"'.format(cwd)\n",
    "%run $tensorflow_helper_func_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.path.dirname(os.getcwd())\n",
    "folder_mod_global = cwd + \"/models/global N-BEATS-exog/\"\n",
    "\n",
    "folder_gen = cwd + \"/generated_data/\"\n",
    "folder_data = cwd + \"/generated_data/dataset_separated_by_ts/\"\n",
    "\n",
    "train_dict = pickle.load(open(folder_gen + \"train_dict.p\",'rb'))\n",
    "val_dict = pickle.load(open(folder_gen + \"val_dict.p\",'rb'))\n",
    "\n",
    "# OTHER\n",
    "freq = \"30min\"\n",
    "periods = 48\n",
    "idx = pd.date_range(\"2009-07-20\", \"2009-12-07\", freq=\"30min\", closed=\"left\")\n",
    "y_cols = [\"H_{}\".format(i) for i in range(1, periods+1)]\n",
    "\n",
    "train_idx = pd.date_range(\"2009-07-27\", \"2010-07-12\", freq=\"30min\", closed=\"left\", name=\"timestamp\")\n",
    "val_idx = pd.date_range(\"2010-07-12\", freq=\"30min\", periods=periods*7*12, name=\"timestamp\")\n",
    "test_idx = pd.date_range(\"2010-10-04\", freq=\"30min\", periods=periods*7*12, name=\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TRAINING\n",
    "- the learning rate is initially set to lr = 0.001 and reduced 3 times by a factor of 10 every time the validation loss plateaus\n",
    "- training works well on CPU instance (it was trained on AWS instance ml.m4.4xlarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# TRAINING PARAMS\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "n_steps_per_epoch = 50\n",
    "epochs = 10_000\n",
    "\n",
    "# MODEL PARAMS\n",
    "params_dict = {\"input_size\": [7*periods, 12+7+48],\n",
    "               \"output_size\": periods,\n",
    "               \"block_layers\": 3,\n",
    "               \"hidden_units\": 512, \n",
    "               \"n_blocks\": 3,\n",
    "               \"block_sharing\": False}\n",
    "\n",
    "# INDICES for all ts\n",
    "ts_ids_list = np.arange(1000).tolist()\n",
    "train_idx_all, train_ids = create_ts_idx(train_idx, ts_ids_list, train_dict)\n",
    "val_idx_all, val_ids = create_ts_idx(val_idx, ts_ids_list, val_dict)\n",
    "test_idx_all, test_ids = create_ts_idx(test_idx, ts_ids_list)\n",
    "\n",
    "# DATA GENERATORS\n",
    "train_generator = TSGenerator(set_type=\"train\",\n",
    "                              batch_size=batch_size, \n",
    "                              n_steps_per_epoch=n_steps_per_epoch,\n",
    "                              ts_ids_list=ts_ids_list)\n",
    "val_generator = TSGenerator(set_type=\"val\", ts_ids_list=ts_ids_list)\n",
    "test_generator = TSGenerator(set_type=\"test\", ts_ids_list=ts_ids_list)\n",
    "\n",
    "# CREATE NN MODEL\n",
    "model = NBeats_exog(params_dict)\n",
    "optimizer = Adam(lr=lr)\n",
    "model.compile(optimizer, loss=\"mae\", metrics=[\"mae\"])\n",
    "              \n",
    "# CALLBACKS\n",
    "csvlogger = CSVLogger(folder_mod_global + 'temp_log.csv')\n",
    "save_val_weights = ModelCheckpoint(folder_mod_global + \"val_best_weights-global.h5\",\n",
    "                                   monitor=\"val_mae\", save_best_only=True)\n",
    "callbacks = [csvlogger] + [save_val_weights]  \n",
    "\n",
    "# FIT\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    verbose=1,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PREDICT & SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAL\n",
    "y_val_pred = model.predict(val_generator)\n",
    "y_val_pred = pd.DataFrame(y_val_pred, index=val_idx_all, columns=y_cols)\n",
    "y_val_pred = pd.concat([val_ids, y_val_pred], axis=1).clip(0)\n",
    "\n",
    "# TEST\n",
    "y_test_pred = model.predict(test_generator)\n",
    "y_test_pred = pd.DataFrame(y_test_pred, index=test_idx_all, columns=y_cols)\n",
    "y_test_pred = pd.concat([test_ids, y_test_pred], axis=1).clip(0)\n",
    "\n",
    "# SAVE\n",
    "y_val_pred.to_pickle(folder_mod_global + \"y_val_pred.p\")\n",
    "y_test_pred.to_pickle(folder_mod_global + \"y_test_pred.p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
