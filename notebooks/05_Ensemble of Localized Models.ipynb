{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK DESCRIPTION\n",
    "The goal of this notebook is to create final ensembles of localized models. Keep in mind that you need a lot of memory for this calculations (we used AWS instance ml.c5.18xlarge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "\n",
    "# DEFAULT\n",
    "tensorflow_helper_func_path =    '\"{}/modules/tensorflow_helper_func.py\"'.format(cwd)\n",
    "%run $tensorflow_helper_func_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clust_preds(n_clusters, set_type=\"test\"):\n",
    "    y_pred = []\n",
    "    for clust_n in range(n_clusters):\n",
    "        file_path = folder_mod_local + \"y_{}_pred/y_{}_pred, n_clusters={}, clust={}.p\".format(set_type, set_type, n_clusters, clust_n)\n",
    "        y_pred_clust = pd.read_pickle(file_path)\n",
    "        y_pred.append(y_pred_clust)\n",
    "\n",
    "    y_pred = pd.concat(y_pred).clip(0)\n",
    "    return y_pred\n",
    "\n",
    "def get_errors(y_true, y_pred, squared=False):\n",
    "    ts_id = y_true.loc[:, \"ts_id\"]\n",
    "    errors = y_true.loc[:, y_true.columns.difference([\"ts_id\"])] - y_pred.loc[:, y_pred.columns.difference([\"ts_id\"])]\n",
    "    \n",
    "    if squared: errors = errors ** 2\n",
    "    \n",
    "    errors = pd.concat([ts_id, errors], axis=1)\n",
    "    return errors\n",
    "\n",
    "def get_mae(errors):\n",
    "    mae_df = errors.abs().groupby(\"ts_id\").mean()\n",
    "    mae_per_ts = mae_df.mean(axis=1)\n",
    "    return mae_per_ts\n",
    "\n",
    "def get_mape(errors, y_true):\n",
    "    ts_id = y_true.loc[:, \"ts_id\"]\n",
    "\n",
    "    pe = 100 * errors.loc[:, errors.columns.difference([\"ts_id\"])].abs() / y_true.loc[:, y_true.columns.difference([\"ts_id\"])]\n",
    "    pe = pd.concat([ts_id, pe.mean(axis=1)], axis=1)\n",
    "\n",
    "    mape_per_ts = pe.groupby(\"ts_id\").mean().iloc[:, 0]\n",
    "    return mape_per_ts\n",
    "\n",
    "def get_nmae(errors, y_true):\n",
    "    mae_df = errors.abs().groupby(\"ts_id\").mean()\n",
    "    mae_per_ts = mae_df.mean(axis=1)\n",
    "    mean_per_ts = y_true.groupby(\"ts_id\").mean().mean(axis=1)\n",
    "    return mae_per_ts / mean_per_ts\n",
    "\n",
    "def mean_per_agg(s):\n",
    "    s_per_agg = pd.Series({\"single\": s.iloc[:250].mean(), \n",
    "                           \"sTS\":    s.iloc[250: 500].mean(),\n",
    "                           \"mTS\":    s.iloc[500: 750].mean(),\n",
    "                           \"lTS\":    s.iloc[750:].mean(),\n",
    "                           \"All\":    s.mean()\n",
    "                          })\n",
    "    return s_per_agg.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_gen = cwd + \"/generated_data/\"\n",
    "folder_mod_global = cwd + \"/models/global N-BEATS-exog/\"\n",
    "folder_mod_local = cwd + \"/models/localized N-BEATS-exog/\"\n",
    "\n",
    "# CLUSTERING RESULTS\n",
    "clust_all = pd.read_csv(folder_gen + \"cluster_data.csv\", index_col=\"ts_id\")\n",
    "clust_all.columns = clust_all.columns.astype(int)\n",
    "\n",
    "# TRUE TARGET VALUES\n",
    "y_val = pd.read_pickle(folder_gen + \"y_val.p\")\n",
    "y_test = pd.read_pickle(folder_gen + \"y_test.p\")\n",
    "\n",
    "# NAIVE PREDICTIONS\n",
    "y_val_pred_naive = pd.read_pickle(folder_gen + \"y_val_pred_naive.p\")\n",
    "y_test_pred_naive = pd.read_pickle(folder_gen + \"y_test_pred_naive.p\")\n",
    "\n",
    "# INITIAL GLOBAL MODEL PREDICTIONS\n",
    "y_val_pred_baseline = pd.read_pickle(folder_mod_global + \"y_val_pred.p\")\n",
    "y_test_pred_baseline = pd.read_pickle(folder_mod_global + \"y_test_pred.p\")\n",
    "\n",
    "periods = 48\n",
    "y_cols = [\"H_{}\".format(i) for i in range(1, periods+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LOAD & EVALUATE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE NAIVE MODEL\n",
    "val_errors_naive = get_errors(y_val, y_val_pred_naive)\n",
    "val_mae_naive = get_mae(val_errors_naive)\n",
    "\n",
    "test_errors_naive = get_errors(y_test, y_test_pred_naive)\n",
    "test_mae_naive = get_mae(test_errors_naive)\n",
    "test_mae_naive_per_horizon = test_errors_naive.abs().groupby(\"ts_id\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_n_clusters = 20\n",
    "\n",
    "y_val_pred_clust_dict, y_test_pred_clust_dict = {}, {}\n",
    "\n",
    "val_mae, test_mae = [], []\n",
    "val_nmae, test_nmae = [], []\n",
    "val_mape, test_mape = [], []\n",
    "val_mase, test_mase = [], []\n",
    "\n",
    "# CREATE PREDICTION DICT & EVALUATION PER TS\n",
    "for n_clusters in range(1, max_n_clusters+1):\n",
    "    print(n_clusters)\n",
    "    \n",
    "    # LOAD PREDICTIONS\n",
    "    if n_clusters == 1:  # load predictions from initial global model\n",
    "        y_val_pred_clust = y_val_pred_baseline\n",
    "        y_test_pred_clust = y_test_pred_baseline\n",
    "        \n",
    "        y_val_pred_clust_dict[n_clusters] = y_val_pred_baseline\n",
    "        y_test_pred_clust_dict[n_clusters] = y_test_pred_baseline\n",
    "        \n",
    "    else:  # load predictions from localized models\n",
    "        y_val_pred_clust = get_clust_preds(n_clusters, set_type=\"val\")\n",
    "        # re-order rows to match y_val\n",
    "        y_val_pred_clust = (y_val.loc[:, [\"ts_id\"]].reset_index()\n",
    "                            .merge(y_val_pred_clust.reset_index(), on=[\"timestamp\", \"ts_id\"])\n",
    "                            .set_index(\"timestamp\"))\n",
    "        \n",
    "        y_test_pred_clust = get_clust_preds(n_clusters, set_type=\"test\")\n",
    "        # re-order rows to match y_test\n",
    "        y_test_pred_clust = (y_test.loc[:, [\"ts_id\"]].reset_index()\n",
    "                            .merge(y_test_pred_clust.reset_index(), on=[\"timestamp\", \"ts_id\"])\n",
    "                            .set_index(\"timestamp\"))\n",
    "            \n",
    "        y_val_pred_clust_dict[n_clusters] = y_val_pred_clust\n",
    "        y_test_pred_clust_dict[n_clusters] = y_test_pred_clust\n",
    "\n",
    "        \n",
    "    # EVALUATION - val\n",
    "    val_errors_clust = get_errors(y_val, y_val_pred_clust)\n",
    "\n",
    "    ## MAE\n",
    "    val_mae_clust = get_mae(val_errors_clust)\n",
    "    val_mae.append(val_mae_clust.rename(n_clusters))\n",
    "\n",
    "    ## NMAE\n",
    "    val_nmae_clust = get_nmae(val_errors_clust, y_val)\n",
    "    val_nmae.append(val_nmae_clust.rename(n_clusters))\n",
    "\n",
    "    ## MAPE\n",
    "    val_mape_clust = get_mape(val_errors_clust, y_val)\n",
    "    val_mape.append(val_mape_clust.rename(n_clusters))\n",
    "\n",
    "    ## MASE\n",
    "    val_mase_clust = val_mae_clust / val_mae_naive\n",
    "    val_mase.append(val_mase_clust.rename(n_clusters))\n",
    "        \n",
    "        \n",
    "    # EVALUATION - test\n",
    "    test_errors_clust = get_errors(y_test, y_test_pred_clust)\n",
    "\n",
    "    ## MAE\n",
    "    test_mae_clust = get_mae(test_errors_clust)\n",
    "    test_mae.append(test_mae_clust.rename(n_clusters))\n",
    "\n",
    "    ## NMAE\n",
    "    test_nmae_clust = get_nmae(test_errors_clust, y_test)\n",
    "    test_nmae.append(test_nmae_clust.rename(n_clusters))\n",
    "\n",
    "    ## MAPE\n",
    "    test_mape_clust = get_mape(test_errors_clust, y_test)\n",
    "    test_mape.append(test_mape_clust.rename(n_clusters))\n",
    "\n",
    "    ## MASE\n",
    "    test_mase_clust = test_mae_clust / test_mae_naive\n",
    "    test_mase.append(test_mase_clust.rename(n_clusters))\n",
    "    \n",
    "val_mae = pd.concat(val_mae, axis=1)\n",
    "val_nmae = pd.concat(val_nmae, axis=1)\n",
    "val_mape = pd.concat(val_mape, axis=1)\n",
    "val_mase = pd.concat(val_mase, axis=1)\n",
    "    \n",
    "test_mae = pd.concat(test_mae, axis=1)\n",
    "test_nmae = pd.concat(test_nmae, axis=1)\n",
    "test_mape = pd.concat(test_mape, axis=1)\n",
    "test_mase = pd.concat(test_mase, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PROPOSED LOCALIZATION WITH ENSEMBLING\n",
    "- create an ensemble from the cluster hierarchy, as proposed in our framework (denoted as ENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_res_all = []\n",
    "\n",
    "for ts_id in y_val.ts_id.unique():\n",
    "    if ts_id % 100 == 0: print(ts_id)\n",
    "       \n",
    "    # create a list of plausible clusters\n",
    "    # clust_list[0] corresponds to initial global model\n",
    "    # clust_list[1] corresponds to using k = 2 when clustering (2 subsets of initial set of time series)\n",
    "    # keep in mind, clust_list is sorted according to performance on VAL set (val_mase) !!!\n",
    "    clust_list = val_mase.loc[ts_id].sort_values().index\n",
    "\n",
    "    # baseline MASE (without ensembling) - same as initial global model\n",
    "    mase_ts_previous = val_mase.loc[ts_id, clust_list[0]]\n",
    "    optim_n_candidates = 1\n",
    "\n",
    "    # DETERMINE BEST CANDIDATES ON VAL\n",
    "    val_mase_ts_list = [mase_ts_previous]\n",
    "\n",
    "    # try different number of candidates\n",
    "    # start with 2 models (initial global model and 2 cluster subsets), \n",
    "    # next 3 models (initial global model and 2 cluster subsets and 3 cluster subsets) etc.\n",
    "    for n_candidates in range(2, len(clust_list)+1):\n",
    "        candidates_list = clust_list[:n_candidates]\n",
    "\n",
    "        ensemble_list = []\n",
    "        for n_clusters in candidates_list:\n",
    "            y_val_pred_ts = y_val_pred_clust_dict[n_clusters]\n",
    "            y_val_pred_ts = y_val_pred_ts.loc[y_val_pred_ts.ts_id == ts_id, \"H_1\":]\n",
    "            ensemble_list.append(y_val_pred_ts)\n",
    "\n",
    "        y_val_pred_ts_ensemble = pd.concat(ensemble_list).resample(\"30min\").mean().dropna()\n",
    "        y_val_pred_ts_ensemble = pd.concat([y_val.loc[y_val.ts_id == ts_id, \"ts_id\"],\n",
    "                                            y_val_pred_ts_ensemble\n",
    "                                           ], axis=1)\n",
    "        \n",
    "        val_mae_ts = (y_val.loc[y_val.ts_id == ts_id, :].iloc[:, 1:] - y_val_pred_ts_ensemble.iloc[:, 1:]).abs().values.mean()\n",
    "        val_mase_ts = val_mae_ts / val_mae_naive.loc[ts_id]\n",
    "        val_mase_ts_list.append(val_mase_ts)\n",
    "\n",
    "        # if MASE improvement > 0.1 % go further, otherwise exit from for loop\n",
    "        if (mase_ts_previous - val_mase_ts) > 0.001: \n",
    "             mase_ts_previous = val_mase_ts\n",
    "        else:\n",
    "            best_candidates_list = candidates_list[:-1]  # save all except the last one\n",
    "            break\n",
    "\n",
    "    # EVALUATE ON TEST (after determining best_candidates_list on VAL set)\n",
    "    ## create ensemble predictions\n",
    "    y_test_pred_ts_ensemble = []\n",
    "\n",
    "    for n_clusters in best_candidates_list:\n",
    "        y_test_pred_ts = y_test_pred_clust_dict[n_clusters]\n",
    "        y_test_pred_ts = y_test_pred_ts.loc[y_test_pred_ts.ts_id == ts_id, \"H_1\":]\n",
    "        y_test_pred_ts_ensemble.append(y_test_pred_ts)\n",
    "\n",
    "    y_test_pred_ts_ensemble = pd.concat(y_test_pred_ts_ensemble).resample(\"30min\").mean()\n",
    "    y_test_pred_ts_ensemble = pd.concat([y_test.loc[y_test.ts_id == ts_id, \"ts_id\"],\n",
    "                                         y_test_pred_ts_ensemble\n",
    "                                        ], axis=1)\n",
    "    \n",
    "    # EVALUATION OF A FINAL MODEL\n",
    "    ## final ensemble predictions for each time series are stored in y_test_pred_ts_ensemble\n",
    "    y_true = y_test.loc[y_test.ts_id == ts_id, :].iloc[:, 1:]\n",
    "    y_pred = y_test_pred_ts_ensemble.iloc[:, 1:]\n",
    "    \n",
    "    ae_ts = (y_true - y_pred).abs()\n",
    "    ape_ts = 100 * ae_ts / y_true\n",
    "    test_mape_ts = ape_ts.values.mean()\n",
    "    \n",
    "    test_mae_ts = ae_ts.values.mean()\n",
    "    test_mase_ts = test_mae_ts / test_mae_naive.loc[ts_id]\n",
    "    \n",
    "    ts_res = pd.Series({\"ts_id\": ts_id, \n",
    "                        \"test_mase\": test_mase_ts,\n",
    "                        \"test_mape\": test_mape_ts,\n",
    "                        \"test_mae\": test_mae_ts, \n",
    "                        \"best_n_candidates\": len(best_candidates_list)})\n",
    "    ts_res_all.append(ts_res)\n",
    "    \n",
    "# SAVE\n",
    "## test MASE, MAPE & MAE together with number of candidates in ensemble are stored in ts_res_all\n",
    "ts_res_all = pd.concat(ts_res_all, axis=1).T\n",
    "ts_res_all = ts_res_all.astype({\"ts_id\": np.int32, \"best_n_candidates\": np.int32})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
