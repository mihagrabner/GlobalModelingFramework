{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK DESCRIPTION\n",
    "The goal of this notebook is to create matrices X & y for each time series separately and save it to .npy files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDERS PATHS\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "folder_in = cwd + \"/input_data/\"\n",
    "folder_gen = cwd + \"/generated_data/\"\n",
    "folder_data = folder_gen + \"dataset_separated_by_ts/\"\n",
    "\n",
    "# DATA\n",
    "data = pd.read_csv(folder_in + \"data.csv\", index_col=\"timestamp\").astype(np.float32)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.columns = data.columns.astype(int)\n",
    "\n",
    "# PARAMS\n",
    "freq_str = \"30min\"\n",
    "freq = 30\n",
    "periods = 48\n",
    "idx = pd.date_range(\"2009-07-20\", \"2010-12-27\", freq=freq_str, closed=\"left\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-07-14 00:00:00</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.592</td>\n",
       "      <td>1.614</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1.384</td>\n",
       "      <td>1.958</td>\n",
       "      <td>...</td>\n",
       "      <td>101.822006</td>\n",
       "      <td>168.178009</td>\n",
       "      <td>262.874023</td>\n",
       "      <td>151.40799</td>\n",
       "      <td>279.238007</td>\n",
       "      <td>125.795998</td>\n",
       "      <td>331.898010</td>\n",
       "      <td>109.206001</td>\n",
       "      <td>114.315994</td>\n",
       "      <td>278.312012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-07-14 00:30:00</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.368</td>\n",
       "      <td>1.372</td>\n",
       "      <td>3.260</td>\n",
       "      <td>...</td>\n",
       "      <td>94.399994</td>\n",
       "      <td>142.209991</td>\n",
       "      <td>214.033997</td>\n",
       "      <td>130.77800</td>\n",
       "      <td>225.395996</td>\n",
       "      <td>102.136002</td>\n",
       "      <td>314.873962</td>\n",
       "      <td>90.020004</td>\n",
       "      <td>91.412003</td>\n",
       "      <td>216.598007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0      1      2      3      4      5      6      7    \\\n",
       "timestamp                                                                     \n",
       "2009-07-14 00:00:00  0.760  0.592  1.614  0.358  0.286  0.140  0.460  0.410   \n",
       "2009-07-14 00:30:00  0.808  0.588  0.566  0.206  0.288  0.252  0.498  0.368   \n",
       "\n",
       "                       8      9    ...         990         991         992  \\\n",
       "timestamp                          ...                                       \n",
       "2009-07-14 00:00:00  1.384  1.958  ...  101.822006  168.178009  262.874023   \n",
       "2009-07-14 00:30:00  1.372  3.260  ...   94.399994  142.209991  214.033997   \n",
       "\n",
       "                           993         994         995         996  \\\n",
       "timestamp                                                            \n",
       "2009-07-14 00:00:00  151.40799  279.238007  125.795998  331.898010   \n",
       "2009-07-14 00:30:00  130.77800  225.395996  102.136002  314.873962   \n",
       "\n",
       "                            997         998         999  \n",
       "timestamp                                                \n",
       "2009-07-14 00:00:00  109.206001  114.315994  278.312012  \n",
       "2009-07-14 00:30:00   90.020004   91.412003  216.598007  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CREATE X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATE DUMMIES\n",
    "month = pd.Series(idx.month.astype(str), index=idx, name=\"month\").apply(lambda x: \"m{}\".format(x))\n",
    "day = pd.Series(idx.dayofweek.astype(str), index=idx, name=\"day\").apply(lambda x: \"d{}\".format(x))\n",
    "hour = pd.Series(idx.strftime(\"%H:%M\"), index=idx, name=\"hour\")\n",
    "\n",
    "month_dummies = pd.get_dummies(month.sort_values()).reindex(idx)\n",
    "day_dummies = pd.get_dummies(day.sort_values()).reindex(idx)\n",
    "hour_dummies = pd.get_dummies(hour.sort_values()).reindex(idx)\n",
    "\n",
    "# RE-ORDER COLUMNS\n",
    "month_dummies_cols = [\"m{}\".format(month) for month in range(1, 13)]\n",
    "day_dummies_cols = [\"d{}\".format(day) for day in range(7)]\n",
    "hour_dummies_cols = pd.date_range(\"2017-1-1\", periods=periods, freq=\"30min\").strftime(\"%H:%M\").tolist()\n",
    "exog_cols = month_dummies_cols + day_dummies_cols + hour_dummies_cols\n",
    "\n",
    "month_dummies = month_dummies.loc[:, month_dummies_cols]\n",
    "day_dummies = day_dummies.loc[:, day_dummies_cols]\n",
    "hour_dummies = hour_dummies.loc[:, hour_dummies_cols]\n",
    "y_cols = [\"H_{}\".format(h) for h in range(1, periods+1)]\n",
    "\n",
    "\n",
    "def get_lags(ts, lag_start, lag_end):\n",
    "    \"\"\" Create rolling window DataFrame from input Series.\n",
    "    \"\"\"\n",
    "    df_all = []\n",
    "\n",
    "    for shift in range(lag_end, lag_start + 1):\n",
    "        df_all.append(ts.shift(-shift).rename(\"lag_{}\".format(shift)))\n",
    "    df_all = pd.concat(df_all, axis=1)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def get_dataset(ts, idx_list):\n",
    "    train_idx, val_idx = idx_list\n",
    "\n",
    "    # create features\n",
    "    lags = get_lags(ts, lag_start=-1, lag_end=-7*periods)\n",
    "\n",
    "    y_reshaped = get_lags(ts, lag_start=periods-1, lag_end=0)\n",
    "    y_reshaped.columns = y_cols\n",
    "\n",
    "    X = pd.concat([month_dummies,\n",
    "                   day_dummies,\n",
    "                   hour_dummies,\n",
    "                   lags,\n",
    "                  ], axis=1)\n",
    "        \n",
    "    # train/val split\n",
    "    X_train, y_reshaped_train = X.loc[train_idx], y_reshaped.loc[train_idx]\n",
    "    X_val, y_reshaped_val = X.loc[val_idx], y_reshaped.loc[val_idx]\n",
    "    \n",
    "    cols = X.columns\n",
    "    lag_cols = cols[cols.str.contains(\"lag\")]\n",
    "\n",
    "    return (X_train.loc[:, lag_cols], X_train.loc[:, exog_cols], \n",
    "            X_val.loc[:, lag_cols],  X_val.loc[:, exog_cols],\n",
    "            y_reshaped_train, \n",
    "            y_reshaped_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Create indices for sampling\n",
    "- as mentioned in TABLE II: <i> Impact of training dataset size on performance</i>, train & val sets are sampled by approx. 12x to reduce training time while still producing good performance\n",
    "- sampling of train & val sets is done by first creating train_dict & val_dict, where dictionary keys are time series ids. E.g. train_dict[0] returns indices for a train set for ts_id = 0, whereas indices hold the information which samples of that particular time series will be used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = pd.date_range(\"2009-07-27\", \"2010-07-12\", freq=\"30min\", closed=\"left\", name=\"timestamp\")\n",
    "val_idx = pd.date_range(\"2010-07-12\", freq=\"30min\", periods=periods*7*12, name=\"timestamp\")\n",
    "test_idx = pd.date_range(\"2010-10-04\", freq=\"30min\", periods=periods*7*12, name=\"timestamp\")\n",
    "trainval_idx = train_idx.append(val_idx)\n",
    "\n",
    "indices_train = np.arange(len(train_idx))\n",
    "indices_val = np.arange(len(val_idx))\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train_dict, val_dict = {}, {}\n",
    "for ts_id in data.columns:\n",
    "    train_dict[ts_id] = np.random.choice(indices_train, size=periods*7*4, replace=False)\n",
    "    val_dict[ts_id] = np.random.choice(indices_val, size=periods*7, replace=False)\n",
    "    \n",
    "pickle.dump(train_dict, open(folder_gen + \"train_dict.p\", 'wb'))\n",
    "pickle.dump(val_dict, open(folder_gen + \"val_dict.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14983,  4430, 11323, 16261,  2252])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Create X & y for each time series separately & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_val_all, y_test_all = [], []\n",
    "y_val_pred_naive_all, y_test_pred_naive_all = [], []\n",
    "\n",
    "for ts_id in data.columns:\n",
    "    \n",
    "    if ts_id % 50 == 0: print(\"writing, ts_id:\", ts_id)\n",
    "    \n",
    "    # GET DATA FOR ONE TIME SERIES\n",
    "    ts = data.loc[:, ts_id]\n",
    "    \n",
    "    # CALCULATE NAIVE MODEL PREDS\n",
    "    ts_naive = get_lags(ts.shift(7*periods), lag_start=periods-1, lag_end=0)  # naive model = values of previous week\n",
    "    ts_naive.columns = y_cols\n",
    "    y_val_pred_naive = ts_naive.loc[val_idx]\n",
    "    y_test_pred_naive = ts_naive.loc[test_idx]\n",
    "    \n",
    "\n",
    "    # CREATE X & y\n",
    "    ## train/val\n",
    "    (X_train_lags, X_train_exog, \n",
    "     X_val_lags, X_val_exog, \n",
    "     y_train, y_val\n",
    "     ) = get_dataset(ts, (train_idx, val_idx))\n",
    "\n",
    "    ## trainval/test\n",
    "    (X_trainval_lags, X_trainval_exog, \n",
    "     X_test_lags, X_test_exog, \n",
    "     y_trainval, y_test\n",
    "     ) = get_dataset(ts, (trainval_idx, test_idx))\n",
    "    \n",
    "    \n",
    "    # USE ONLY A SUBSET OF ORIGINAL SAMPLES FOR TRAIN & VAL\n",
    "    ## train\n",
    "    X_train_lags = X_train_lags.iloc[train_dict[ts_id]]\n",
    "    X_train_exog = X_train_exog.iloc[train_dict[ts_id]]\n",
    "    y_train = y_train.iloc[train_dict[ts_id]]\n",
    "    \n",
    "    ## val\n",
    "    X_val_lags = X_val_lags.iloc[val_dict[ts_id]]\n",
    "    X_val_exog = X_val_exog.iloc[val_dict[ts_id]]\n",
    "    y_val = y_val.iloc[val_dict[ts_id]]\n",
    "    y_val_pred_naive = y_val_pred_naive.iloc[val_dict[ts_id]]\n",
    "\n",
    "    # SAVE\n",
    "    ## train\n",
    "    np.save(folder_data + \"X_train_ts_id={}, lags.npy\".format(ts_id), X_train_lags)        \n",
    "    np.save(folder_data + \"X_train_ts_id={}, exog.npy\".format(ts_id), X_train_exog)        \n",
    "    np.save(folder_data + \"y_train_ts_id={}.npy\".format(ts_id), y_train)\n",
    "\n",
    "    ## val\n",
    "    np.save(folder_data + \"X_val_ts_id={}, lags.npy\".format(ts_id), X_val_lags)         \n",
    "    np.save(folder_data + \"X_val_ts_id={}, exog.npy\".format(ts_id), X_val_exog)      \n",
    "    np.save(folder_data + \"y_val_ts_id={}.npy\".format(ts_id), y_val)        \n",
    "\n",
    "    ## test\n",
    "    np.save(folder_data + \"X_test_ts_id={}, lags.npy\".format(ts_id), X_test_lags)   \n",
    "    np.save(folder_data + \"X_test_ts_id={}, exog.npy\".format(ts_id), X_test_exog)        \n",
    "    np.save(folder_data + \"y_test_ts_id={}.npy\".format(ts_id), y_test)\n",
    "    \n",
    "    y_val_pred_naive = y_val_pred_naive.assign(ts_id=ts_id)\n",
    "    y_val_pred_naive_all.append(y_val_pred_naive)\n",
    "    \n",
    "    y_test_pred_naive = y_test_pred_naive.assign(ts_id=ts_id)\n",
    "    y_test_pred_naive_all.append(y_test_pred_naive)\n",
    "    \n",
    "    y_val_ts = y_val.assign(ts_id=ts_id)\n",
    "    y_val_all.append(y_val_ts)\n",
    "\n",
    "    y_test_ts = y_test.assign(ts_id=ts_id)\n",
    "    y_test_all.append(y_test_ts)\n",
    "    \n",
    "\n",
    "y_val_pred_naive_all = pd.concat(y_val_pred_naive_all).loc[:, [\"ts_id\"] + y_cols]\n",
    "y_test_pred_naive_all = pd.concat(y_test_pred_naive_all).loc[:, [\"ts_id\"] + y_cols]\n",
    "\n",
    "y_val_all = pd.concat(y_val_all).loc[:, [\"ts_id\"] + y_cols]\n",
    "y_test_all = pd.concat(y_test_all).loc[:, [\"ts_id\"] + y_cols]\n",
    "\n",
    "# SAVE\n",
    "y_val_pred_naive_all.to_pickle(folder_gen + \"y_val_pred_naive.p\")\n",
    "y_test_pred_naive_all.to_pickle(folder_gen + \"y_test_pred_naive.p\")\n",
    "\n",
    "y_val_all.to_pickle(folder_gen + \"y_val.p\")\n",
    "y_test_all.to_pickle(folder_gen + \"y_test.p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
